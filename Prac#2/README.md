# Практична робота №2: Робота з великими наборами даних
# Варіант 14: Групування та apply. Використайте .groupby().apply() для застосування власної функції до груп даних.

## Опис
Цей проєкт демонструє оптимізацію роботи з великими датасетами у Pandas. Він включає:
- Генерацію великого датасету (20 млн рядків).
- Збереження його у форматах CSV та Parquet.
- Зчитування, оптимізацію та аналіз даних.
- Використання `.apply()` та `groupby().apply()` для обробки значень.
- Порівняння швидкості різних підходів для оптимізації.

## 1. Генерація датасету
Функція `generate_dataset()` створює 20 мільйонів рядків з випадковими значеннями для наступних колонок:
- `id` (унікальний ідентифікатор)
- `category` (категорія A, B, C, D)
- `value` (випадкове число від 1 до 1000)
- `genre` (жанр, обраний випадково з 7 варіантів)
- `car_manufacturer` (марка автомобіля, обрана випадково з 8 варіантів)
- `date` (послідовні часові мітки з інтервалом у хвилину)
- `sensor` (випадкове число з нормального розподілу)

Згенерований датасет зберігається у форматах CSV та Parquet для подальшого аналізу.

## 2. Читання та оптимізація датасету
- `read_csv()` завантажує CSV-файл з використанням `chunksize=100_000` для зменшення споживання пам'яті. Під час написання роботи для цієї функції, в цілях оптимізації, було застосовано кешування через `@memory.cache` з бібліотеки `joblib`
- `optimize_dataframe(df)`: перетворює відповідні колонки у тип `category`, що суттєво зменшує споживання пам'яті, та приводить `date` до формату `datetime`.

## 3. Використання `.apply()` для обробки даних
Функція `apply_testing(df)` порівнює три підходи для обчислення значень колонки `value`:
1. `.apply(custom_function)`: застосування функції до кожного елемента (повільний метод).
2. `np.vectorize(custom_function)`: прискорена версія `.apply()`.
3. Використання чистого NumPy (найшвидший варіант).

## 4. Використання `groupby().apply()`
Функція `groupby_apply(df)`:
- Використовує `.groupby().apply(custom_function)`, щоб обчислити нові значення для групованих даних за `car_manufacturer`.
- Виводить по 5 рядків для кожного виробника авто.

## 5. Аналіз великих даних через `groupby()`
- `expensive_computations(df)`: рахує суму `sensor` для `car_manufacturer` та `genre` через `groupby()`.

## Висновок
- Оптимізація пам'яті через `astype("category")` значно покращує продуктивність.
- Використання NumPy замість `.apply()` дає значне прискорення обчислень.
- `groupby().apply()` зручний, але не завжди найефективніший метод.
- Збереження у Parquet забезпечує ефективне зчитування великих даних.

## Запуск
1. Запустіть `generate_dataset()` для створення файлу `large_dataset.csv`.
2. Виконайте основний скрипт для аналізу та оптимізації даних.

